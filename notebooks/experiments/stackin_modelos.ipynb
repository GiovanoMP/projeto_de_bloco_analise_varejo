{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q mlxtend\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diretório configurado com sucesso!\n",
      "Dados carregados com sucesso! Total de registros: 460163\n",
      "Total de clientes identificados: 4196\n",
      "Data de referência para análise: 10/12/2011\n",
      "Dados RFM preparados com sucesso! Total de clientes: 4196\n",
      "\n",
      "Resumo das métricas RFM:\n",
      "          Recencia   Frequencia  ValorMonetario\n",
      "count  4196.000000  4196.000000     4196.000000\n",
      "mean     92.400143     4.031935     1081.733072\n",
      "std      99.874731     7.060200     2337.948654\n",
      "min       1.000000     1.000000        1.900000\n",
      "25%      18.000000     1.000000      218.070000\n",
      "50%      51.000000     2.000000      497.290000\n",
      "75%     144.000000     4.000000     1193.500000\n",
      "max     374.000000   198.000000    91204.820000\n",
      "\n",
      "Iniciando tratamento de outliers...\n",
      "\n",
      "Tratamento de outliers para Recencia:\n",
      "Limite inferior: 1.00\n",
      "Limite superior: 369.00\n",
      "\n",
      "Tratamento de outliers para Frequencia:\n",
      "Limite inferior: 1.00\n",
      "Limite superior: 26.00\n",
      "\n",
      "Tratamento de outliers para ValorMonetario:\n",
      "Limite inferior: 20.80\n",
      "Limite superior: 7945.49\n",
      "\n",
      "Aplicando transformação logarítmica...\n",
      "\n",
      "Normalizando dados...\n",
      "\n",
      "Validação das transformações:\n",
      "Média próxima a 0 e desvio padrão próximo a 1 indicam normalização correta\n",
      "           Recencia    Frequencia  ValorMonetario\n",
      "count  4.196000e+03  4.196000e+03    4.196000e+03\n",
      "mean  -3.784707e-16  4.809203e-16    1.016029e-16\n",
      "std    1.000119e+00  1.000119e+00    1.000119e+00\n",
      "min   -2.351021e+00 -9.687874e-01   -2.606189e+00\n",
      "25%   -6.649257e-01 -9.687874e-01   -6.941124e-01\n",
      "50%    8.911618e-02 -3.401774e-01   -1.314173e-02\n",
      "75%    8.571524e-01  4.517777e-01    7.113408e-01\n",
      "max    1.558742e+00  3.066275e+00    2.281619e+00\n",
      "\n",
      "Pré-processamento concluído com sucesso!\n",
      "\n",
      "Iniciando processo de clusterização...\n",
      "\n",
      "Avaliando qualidade dos clusters...\n",
      "Coeficiente de Silhueta: 0.333\n",
      "\n",
      "Calculando métricas adicionais...\n",
      "\n",
      "Analisando características dos clusters...\n",
      "\n",
      "Análise detalhada por segmento:\n",
      "\n",
      "Segmento: Champions\n",
      "Quantidade de clientes: 1134 (27.0%)\n",
      "\n",
      "Métricas médias:\n",
      "- Recência: 73.2 dias\n",
      "- Frequência: 3.8 compras\n",
      "- Valor médio: R$ 1,104.28\n",
      "- Ticket médio: R$ 342.56\n",
      "\n",
      "Segmento: Leais\n",
      "Quantidade de clientes: 773 (18.4%)\n",
      "\n",
      "Métricas médias:\n",
      "- Recência: 13.0 dias\n",
      "- Frequência: 10.6 compras\n",
      "- Valor médio: R$ 3,008.22\n",
      "- Ticket médio: R$ 304.41\n",
      "\n",
      "Segmento: Em Risco\n",
      "Quantidade de clientes: 848 (20.2%)\n",
      "\n",
      "Métricas médias:\n",
      "- Recência: 21.9 dias\n",
      "- Frequência: 1.9 compras\n",
      "- Valor médio: R$ 363.87\n",
      "- Ticket médio: R$ 214.74\n",
      "\n",
      "Segmento: Perdidos\n",
      "Quantidade de clientes: 1441 (34.3%)\n",
      "\n",
      "Métricas médias:\n",
      "- Recência: 191.5 dias\n",
      "- Frequência: 1.3 compras\n",
      "- Valor médio: R$ 234.11\n",
      "- Ticket médio: R$ 200.73\n",
      "\n",
      "Clusterização e análise concluídas com sucesso!\n",
      "\n",
      "Iniciando análise de Market Basket por segmento...\n",
      "\n",
      "Processando Cluster 1.0 (Leais)...\n",
      "Criando matriz de cestas para o segmento Leais...\n",
      "Produtos após filtragem: 1881\n",
      "Regras geradas para Leais: 2422\n",
      "\n",
      "Processando Cluster 0.0 (Champions)...\n",
      "Criando matriz de cestas para o segmento Champions...\n",
      "Produtos após filtragem: 1314\n",
      "Regras geradas para Champions: 1550\n",
      "\n",
      "Processando Cluster 3.0 (Perdidos)...\n",
      "Criando matriz de cestas para o segmento Perdidos...\n",
      "Produtos após filtragem: 371\n",
      "Regras geradas para Perdidos: 132\n",
      "\n",
      "Processando Cluster 2.0 (Em Risco)...\n",
      "Criando matriz de cestas para o segmento Em Risco...\n",
      "Produtos após filtragem: 458\n",
      "Regras geradas para Em Risco: 300\n",
      "\n",
      "Total de regras geradas: 4404\n",
      "\n",
      "Melhores regras por segmento:\n",
      "\n",
      "Top 3 regras para Leais:\n",
      "Se compra ['REGENCY TEA PLATE GREEN ', 'REGENCY TEA PLATE ROSES ']\n",
      "Também compra ['REGENCY TEA PLATE PINK']\n",
      "Confiança: 78.81%\n",
      "Lift: 61.88\n",
      "\n",
      "Se compra ['REGENCY TEA PLATE PINK']\n",
      "Também compra ['REGENCY TEA PLATE GREEN ', 'REGENCY TEA PLATE ROSES ']\n",
      "Confiança: 79.49%\n",
      "Lift: 61.88\n",
      "\n",
      "Se compra ['REGENCY TEA PLATE ROSES ', 'REGENCY TEA PLATE PINK']\n",
      "Também compra ['REGENCY TEA PLATE GREEN ']\n",
      "Confiança: 91.18%\n",
      "Lift: 61.13\n",
      "\n",
      "\n",
      "Top 3 regras para Champions:\n",
      "Se compra ['HERB MARKER PARSLEY', 'HERB MARKER THYME', 'HERB MARKER ROSEMARY']\n",
      "Também compra ['HERB MARKER CHIVES ', 'HERB MARKER MINT', 'HERB MARKER BASIL']\n",
      "Confiança: 83.02%\n",
      "Lift: 75.65\n",
      "\n",
      "Se compra ['HERB MARKER CHIVES ', 'HERB MARKER THYME', 'HERB MARKER BASIL']\n",
      "Também compra ['HERB MARKER PARSLEY', 'HERB MARKER MINT', 'HERB MARKER ROSEMARY']\n",
      "Confiança: 93.62%\n",
      "Lift: 75.65\n",
      "\n",
      "Se compra ['HERB MARKER PARSLEY', 'HERB MARKER MINT', 'HERB MARKER ROSEMARY']\n",
      "Também compra ['HERB MARKER CHIVES ', 'HERB MARKER THYME', 'HERB MARKER BASIL']\n",
      "Confiança: 83.02%\n",
      "Lift: 75.65\n",
      "\n",
      "\n",
      "Top 3 regras para Perdidos:\n",
      "Se compra [\"POPPY'S PLAYHOUSE KITCHEN\", \"POPPY'S PLAYHOUSE BEDROOM \"]\n",
      "Também compra [\"POPPY'S PLAYHOUSE LIVINGROOM \"]\n",
      "Confiança: 76.00%\n",
      "Lift: 49.83\n",
      "\n",
      "Se compra [\"POPPY'S PLAYHOUSE LIVINGROOM \"]\n",
      "Também compra [\"POPPY'S PLAYHOUSE KITCHEN\", \"POPPY'S PLAYHOUSE BEDROOM \"]\n",
      "Confiança: 67.86%\n",
      "Lift: 49.83\n",
      "\n",
      "Se compra [\"POPPY'S PLAYHOUSE KITCHEN\", \"POPPY'S PLAYHOUSE LIVINGROOM \"]\n",
      "Também compra [\"POPPY'S PLAYHOUSE BEDROOM \"]\n",
      "Confiança: 86.36%\n",
      "Lift: 48.05\n",
      "\n",
      "\n",
      "Top 3 regras para Em Risco:\n",
      "Se compra ['SET/10 BLUE POLKADOT PARTY CANDLES']\n",
      "Também compra ['SET/10 PINK POLKADOT PARTY CANDLES']\n",
      "Confiança: 85.71%\n",
      "Lift: 57.61\n",
      "\n",
      "Se compra ['SET/10 PINK POLKADOT PARTY CANDLES']\n",
      "Também compra ['SET/10 BLUE POLKADOT PARTY CANDLES']\n",
      "Confiança: 75.00%\n",
      "Lift: 57.61\n",
      "\n",
      "Se compra ['HAPPY STENCIL CRAFT']\n",
      "Também compra ['MONSTERS STENCIL CRAFT']\n",
      "Confiança: 70.97%\n",
      "Lift: 44.03\n",
      "\n",
      "\n",
      "Análise de Market Basket concluída!\n",
      "\n",
      "Iniciando geração de insights e salvamento do modelo...\n",
      "Diretórios criados/verificados com sucesso!\n",
      "\n",
      "Gerando relatório de insights...\n",
      "Relatório de insights gerado com sucesso!\n",
      "\n",
      "Iniciando processo de salvamento do modelo e metadata...\n",
      "Modelo K-means salvo com sucesso!\n",
      "Scaler salvo com sucesso!\n",
      "Metadata do modelo salva com sucesso!\n",
      "\n",
      "Realizando testes de carregamento...\n",
      "Testes de carregamento realizados com sucesso!\n",
      "Número de clusters do modelo carregado: 4\n",
      "\n",
      "=== RESUMO DA EXECUÇÃO ===\n",
      "Total de registros processados: 460163\n",
      "Total de clientes segmentados: 4196\n",
      "Arquivo de insights gerado: documents/analise_detalhada_clientes.txt\n",
      "Modelos e metadata salvos em: /models/\n",
      "\n",
      "Processo concluído com sucesso!\n"
     ]
    }
   ],
   "source": [
    "# === IMPORTS E CONFIGURAÇÕES ===\n",
    "# Bibliotecas principais para manipulação de dados e análise\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "# Bibliotecas para visualização\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Bibliotecas para machine learning\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# Biblioteca para análise de market basket\n",
    "from mlxtend.frequent_patterns import fpgrowth, association_rules\n",
    "\n",
    "# Biblioteca para salvar/carregar modelos\n",
    "from joblib import dump, load\n",
    "\n",
    "# Configurações de sistema e warnings\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configuração de visualização\n",
    "sns.set(style='whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "\n",
    "\n",
    "# Navegar para o diretório de dados processados\n",
    "try:\n",
    "    os.chdir('../data/processed')\n",
    "    print(\"Diretório configurado com sucesso!\")\n",
    "except Exception as e:\n",
    "    print(f\"Erro ao configurar diretório: {e}\")\n",
    "\n",
    "# === CARREGAMENTO E PREPARAÇÃO DOS DADOS ===\n",
    "\n",
    "# 1. Carregar dados\n",
    "try:\n",
    "    # Carrega CSV com parsing de datas automático\n",
    "    df = pd.read_csv('transacoes_final.csv', parse_dates=['DataFatura'])\n",
    "    print(f\"Dados carregados com sucesso! Total de registros: {len(df)}\")\n",
    "except Exception as e:\n",
    "    print(f\"Erro ao carregar dados: {e}\")\n",
    "\n",
    "# 2. Verificar e calcular valor total da fatura se necessário\n",
    "if 'ValorTotalFatura' not in df.columns:\n",
    "    df['ValorTotalFatura'] = df['Quantidade'] * df['PrecoUnitario']\n",
    "    print(\"Coluna ValorTotalFatura criada!\")\n",
    "\n",
    "# 3. Preparação dos dados para análise RFM\n",
    "# Converter e limpar IDs de cliente\n",
    "df['IDCliente'] = df['IDCliente'].astype(str)\n",
    "\n",
    "# Filtrar apenas clientes identificados\n",
    "df_clientes = df[(df['IDCliente'].notna()) & (df['IDCliente'] != 'Desconhecido')]\n",
    "print(f\"Total de clientes identificados: {df_clientes['IDCliente'].nunique()}\")\n",
    "\n",
    "# 4. Definir data de referência para cálculo de recência\n",
    "# Usamos o dia após a última compra como referência\n",
    "data_referencia = df_clientes['DataFatura'].max() + pd.DateOffset(days=1)\n",
    "print(f\"Data de referência para análise: {data_referencia.strftime('%d/%m/%Y')}\")\n",
    "\n",
    "# 5. Criar dataframe RFM base\n",
    "# Calcular métricas agregadas por cliente\n",
    "rfm = df_clientes.groupby('IDCliente').agg({\n",
    "    'DataFatura': lambda x: (data_referencia - x.max()).days,  # Recência\n",
    "    'ValorTotalFatura': 'sum'  # Valor Monetário\n",
    "}).reset_index()\n",
    "\n",
    "# Renomear colunas para clareza\n",
    "rfm.columns = ['IDCliente', 'Recencia', 'ValorMonetario']\n",
    "\n",
    "# Calcular frequência separadamente (número único de faturas)\n",
    "frequencia = df_clientes.groupby('IDCliente')['NumeroFatura'].nunique().reset_index()\n",
    "frequencia.columns = ['IDCliente', 'Frequencia']\n",
    "\n",
    "# Juntar todas as métricas RFM\n",
    "rfm = rfm.merge(frequencia, on='IDCliente')\n",
    "\n",
    "# 6. Limpeza e conversão de tipos\n",
    "# Converter métricas para numérico e tratar valores ausentes\n",
    "for coluna in ['Recencia', 'Frequencia', 'ValorMonetario']:\n",
    "    rfm[coluna] = pd.to_numeric(rfm[coluna], errors='coerce')\n",
    "\n",
    "# Remover registros com valores ausentes\n",
    "rfm = rfm.dropna(subset=['Recencia', 'Frequencia', 'ValorMonetario'])\n",
    "print(f\"Dados RFM preparados com sucesso! Total de clientes: {len(rfm)}\")\n",
    "\n",
    "# 7. Validação dos dados\n",
    "print(\"\\nResumo das métricas RFM:\")\n",
    "print(rfm[['Recencia', 'Frequencia', 'ValorMonetario']].describe())\n",
    "\n",
    "# === TRATAMENTO DE OUTLIERS E NORMALIZAÇÃO ===\n",
    "\n",
    "# 1. Tratamento de Outliers\n",
    "print(\"\\nIniciando tratamento de outliers...\")\n",
    "\n",
    "# Armazenar limites originais para documentação\n",
    "limites_originais = {}\n",
    "for col in ['Recencia', 'Frequencia', 'ValorMonetario']:\n",
    "    limites_originais[col] = {\n",
    "        'min': rfm[col].min(),\n",
    "        'max': rfm[col].max(),\n",
    "        'mean': rfm[col].mean(),\n",
    "        'std': rfm[col].std()\n",
    "    }\n",
    "\n",
    "# Aplicar clipping nos percentis 1 e 99\n",
    "for col in ['Recencia', 'Frequencia', 'ValorMonetario']:\n",
    "    # Calcular limites\n",
    "    lower = rfm[col].quantile(0.01)\n",
    "    upper = rfm[col].quantile(0.99)\n",
    "    \n",
    "    # Armazenar limites para documentação\n",
    "    limites_tratados = {\n",
    "        'lower': lower,\n",
    "        'upper': upper\n",
    "    }\n",
    "    \n",
    "    # Aplicar clipping\n",
    "    rfm[col] = rfm[col].clip(lower=lower, upper=upper)\n",
    "    \n",
    "    print(f\"\\nTratamento de outliers para {col}:\")\n",
    "    print(f\"Limite inferior: {lower:.2f}\")\n",
    "    print(f\"Limite superior: {upper:.2f}\")\n",
    "\n",
    "# 2. Transformação logarítmica\n",
    "print(\"\\nAplicando transformação logarítmica...\")\n",
    "# Aplicar log1p (log(1+x)) para lidar com valores zero\n",
    "rfm_log = rfm[['Recencia', 'Frequencia', 'ValorMonetario']].apply(np.log1p)\n",
    "\n",
    "# 3. Normalização com StandardScaler\n",
    "print(\"\\nNormalizando dados...\")\n",
    "# Inicializar e ajustar o scaler\n",
    "scaler = StandardScaler()\n",
    "rfm_normalizado = scaler.fit_transform(rfm_log)\n",
    "\n",
    "# Criar DataFrame normalizado para visualização\n",
    "rfm_normalizado_df = pd.DataFrame(\n",
    "    rfm_normalizado,\n",
    "    columns=['Recencia', 'Frequencia', 'ValorMonetario'],\n",
    "    index=rfm.index\n",
    ")\n",
    "\n",
    "# 4. Validação das transformações\n",
    "print(\"\\nValidação das transformações:\")\n",
    "print(\"Média próxima a 0 e desvio padrão próximo a 1 indicam normalização correta\")\n",
    "print(rfm_normalizado_df.describe())\n",
    "\n",
    "# 5. Armazenar informações de pré-processamento\n",
    "info_preprocessamento = {\n",
    "    'limites_originais': limites_originais,\n",
    "    'limites_tratados': limites_tratados,\n",
    "    'transformacoes_aplicadas': ['log1p', 'StandardScaler'],\n",
    "    'scaler_mean_': scaler.mean_.tolist(),\n",
    "    'scaler_scale_': scaler.scale_.tolist()\n",
    "}\n",
    "\n",
    "print(\"\\nPré-processamento concluído com sucesso!\")\n",
    "\n",
    "# === CLUSTERIZAÇÃO E ANÁLISE DOS SEGMENTOS ===\n",
    "\n",
    "# 1. Aplicação do K-means\n",
    "print(\"\\nIniciando processo de clusterização...\")\n",
    "# Definir número de clusters\n",
    "k = 4\n",
    "kmeans = KMeans(\n",
    "    n_clusters=k,\n",
    "    random_state=42,\n",
    "    n_init=10  # Número de vezes que o algoritmo será executado com diferentes centróides iniciais\n",
    ")\n",
    "\n",
    "# Treinar o modelo\n",
    "kmeans.fit(rfm_normalizado)\n",
    "\n",
    "# Adicionar labels ao DataFrame original\n",
    "rfm['Cluster'] = kmeans.labels_\n",
    "\n",
    "# 2. Avaliação do Modelo\n",
    "print(\"\\nAvaliando qualidade dos clusters...\")\n",
    "# Calcular pontuação de silhueta\n",
    "silhouette_avg = silhouette_score(rfm_normalizado, kmeans.labels_)\n",
    "print(f\"Coeficiente de Silhueta: {silhouette_avg:.3f}\")\n",
    "\n",
    "# 3. Cálculo de Métricas Adicionais\n",
    "print(\"\\nCalculando métricas adicionais...\")\n",
    "# Adicionar métricas derivadas\n",
    "rfm['UltimaCompra'] = data_referencia - pd.to_timedelta(rfm['Recencia'], unit='D')\n",
    "rfm['CompraMediaPorVisita'] = rfm['ValorMonetario'] / rfm['Frequencia']\n",
    "rfm['DiasEntreCompras'] = rfm['Recencia'] / rfm['Frequencia']\n",
    "\n",
    "# 4. Análise dos Clusters\n",
    "print(\"\\nAnalisando características dos clusters...\")\n",
    "# Calcular métricas médias por cluster\n",
    "cluster_metrics = rfm.groupby('Cluster')[\n",
    "    ['Recencia', 'Frequencia', 'ValorMonetario', 'CompraMediaPorVisita', 'DiasEntreCompras']\n",
    "].agg(['mean', 'count', 'std']).round(2)\n",
    "\n",
    "# 5. Nomeação dos Clusters\n",
    "# Definir nomes dos clusters baseados em múltiplas métricas\n",
    "cluster_names = {\n",
    "    0: 'Champions',\n",
    "    1: 'Leais',\n",
    "    2: 'Em Risco',\n",
    "    3: 'Perdidos'\n",
    "}\n",
    "\n",
    "# Adicionar nomes dos segmentos ao DataFrame\n",
    "rfm['Segmento'] = rfm['Cluster'].map(cluster_names)\n",
    "\n",
    "# 6. Análise Detalhada dos Segmentos\n",
    "print(\"\\nAnálise detalhada por segmento:\")\n",
    "for cluster in sorted(cluster_names.keys()):\n",
    "    segment_data = rfm[rfm['Cluster'] == cluster]\n",
    "    nome_segmento = cluster_names[cluster]\n",
    "    \n",
    "    print(f\"\\nSegmento: {nome_segmento}\")\n",
    "    print(f\"Quantidade de clientes: {len(segment_data)} ({len(segment_data)/len(rfm)*100:.1f}%)\")\n",
    "    print(\"\\nMétricas médias:\")\n",
    "    print(f\"- Recência: {segment_data['Recencia'].mean():.1f} dias\")\n",
    "    print(f\"- Frequência: {segment_data['Frequencia'].mean():.1f} compras\")\n",
    "    print(f\"- Valor médio: R$ {segment_data['ValorMonetario'].mean():,.2f}\")\n",
    "    print(f\"- Ticket médio: R$ {segment_data['CompraMediaPorVisita'].mean():,.2f}\")\n",
    "\n",
    "# 7. Integrar resultados aos dados originais\n",
    "df = df.merge(rfm[['IDCliente', 'Cluster', 'Segmento']], on='IDCliente', how='left')\n",
    "\n",
    "# 8. Armazenar métricas do modelo\n",
    "metricas_modelo = {\n",
    "    'silhouette_score': silhouette_avg,\n",
    "    'inertia': kmeans.inertia_,\n",
    "    'distribuicao_clusters': rfm['Cluster'].value_counts().to_dict(),\n",
    "    'metricas_por_cluster': cluster_metrics.to_dict()\n",
    "}\n",
    "\n",
    "print(\"\\nClusterização e análise concluídas com sucesso!\")\n",
    "\n",
    "# === ANÁLISE DE MARKET BASKET POR SEGMENTO ===\n",
    "\n",
    "print(\"\\nIniciando análise de Market Basket por segmento...\")\n",
    "\n",
    "# 1. Preparação para análise de Market Basket\n",
    "regras_clusters = []  # Lista para armazenar regras de cada cluster\n",
    "metricas_market_basket = {}  # Dicionário para armazenar métricas\n",
    "\n",
    "# 2. Análise por cluster\n",
    "for cluster in df['Cluster'].dropna().unique():\n",
    "    print(f\"\\nProcessando Cluster {cluster} ({cluster_names[cluster]})...\")\n",
    "    \n",
    "    # Filtrar dados do cluster\n",
    "    df_cluster = df[df['Cluster'] == cluster]\n",
    "    df_cluster = df_cluster.dropna(subset=['NumeroFatura', 'Descricao'])\n",
    "    \n",
    "    # Criar identificador único de transação\n",
    "    df_cluster['TransacaoID'] = df_cluster['NumeroFatura'].astype(str) + '_' + df_cluster['IDCliente'].astype(str)\n",
    "    \n",
    "    # Criar matriz de cestas de compras\n",
    "    print(f\"Criando matriz de cestas para o segmento {cluster_names[cluster]}...\")\n",
    "    basket_cluster = df_cluster.groupby(['TransacaoID', 'Descricao'])['Quantidade'].sum().unstack().fillna(0)\n",
    "    \n",
    "    # Converter para matriz binária (0 ou 1)\n",
    "    basket_cluster = basket_cluster.applymap(lambda x: 1 if x >= 1 else 0)\n",
    "    \n",
    "    # Armazenar métricas iniciais\n",
    "    metricas_market_basket[cluster] = {\n",
    "        'total_transacoes': len(basket_cluster),\n",
    "        'total_produtos': len(basket_cluster.columns)\n",
    "    }\n",
    "    \n",
    "    # Filtrar produtos frequentes (mínimo de 20 ocorrências)\n",
    "    item_counts = basket_cluster.sum(axis=0)\n",
    "    basket_cluster = basket_cluster.loc[:, item_counts >= 20]\n",
    "    \n",
    "    print(f\"Produtos após filtragem: {len(basket_cluster.columns)}\")\n",
    "    \n",
    "    # 3. Geração de regras de associação\n",
    "    if len(basket_cluster.columns) > 1:\n",
    "        try:\n",
    "            # Encontrar conjuntos frequentes\n",
    "            frequent_itemsets = fpgrowth(\n",
    "                basket_cluster,\n",
    "                min_support=0.01,  # Suporte mínimo de 1%\n",
    "                use_colnames=True\n",
    "            )\n",
    "            \n",
    "            # Gerar regras de associação\n",
    "            regras = association_rules(\n",
    "                frequent_itemsets,\n",
    "                metric='confidence',\n",
    "                min_threshold=0.1  # Confiança mínima de 10%\n",
    "            )\n",
    "            \n",
    "            # Adicionar informações do cluster\n",
    "            regras['Cluster'] = cluster\n",
    "            regras['Segmento'] = cluster_names[cluster]\n",
    "            \n",
    "            # Armazenar métricas\n",
    "            metricas_market_basket[cluster].update({\n",
    "                'total_itemsets': len(frequent_itemsets),\n",
    "                'total_regras': len(regras),\n",
    "                'confidence_media': regras['confidence'].mean(),\n",
    "                'lift_medio': regras['lift'].mean()\n",
    "            })\n",
    "            \n",
    "            # Adicionar à lista de regras\n",
    "            regras_clusters.append(regras)\n",
    "            \n",
    "            print(f\"Regras geradas para {cluster_names[cluster]}: {len(regras)}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Erro na geração de regras para cluster {cluster}: {e}\")\n",
    "    else:\n",
    "        print(f\"Produtos insuficientes para análise no cluster {cluster}\")\n",
    "\n",
    "# 4. Consolidar todas as regras\n",
    "if regras_clusters:\n",
    "    regras_todas = pd.concat(regras_clusters, ignore_index=True)\n",
    "    print(f\"\\nTotal de regras geradas: {len(regras_todas)}\")\n",
    "    \n",
    "    # 5. Análise das melhores regras por segmento\n",
    "    print(\"\\nMelhores regras por segmento:\")\n",
    "    for segmento in regras_todas['Segmento'].unique():\n",
    "        regras_segmento = regras_todas[regras_todas['Segmento'] == segmento]\n",
    "        print(f\"\\nTop 3 regras para {segmento}:\")\n",
    "        top_regras = regras_segmento.nlargest(3, 'lift')\n",
    "        for _, regra in top_regras.iterrows():\n",
    "            print(f\"Se compra {list(regra['antecedents'])}\")\n",
    "            print(f\"Também compra {list(regra['consequents'])}\")\n",
    "            print(f\"Confiança: {regra['confidence']:.2%}\")\n",
    "            print(f\"Lift: {regra['lift']:.2f}\\n\")\n",
    "\n",
    "else:\n",
    "    print(\"Nenhuma regra de associação foi gerada\")\n",
    "\n",
    "print(\"\\nAnálise de Market Basket concluída!\")\n",
    "\n",
    "# === GERAÇÃO DE INSIGHTS E SALVAMENTO DO MODELO ===\n",
    "\n",
    "print(\"\\nIniciando geração de insights e salvamento do modelo...\")\n",
    "\n",
    "# 1. Preparar diretórios\n",
    "try:\n",
    "    # Voltar para o diretório principal e criar pastas necessárias\n",
    "    os.chdir('../../')\n",
    "    os.makedirs('models', exist_ok=True)\n",
    "    os.makedirs('documents', exist_ok=True)\n",
    "    print(\"Diretórios criados/verificados com sucesso!\")\n",
    "except Exception as e:\n",
    "    print(f\"Erro ao criar diretórios: {e}\")\n",
    "\n",
    "# 2. Gerar relatório de insights\n",
    "print(\"\\nGerando relatório de insights...\")\n",
    "with open('documents/analise_detalhada_clientes.txt', 'w', encoding='utf-8') as f:\n",
    "    # Cabeçalho\n",
    "    f.write(\"=== ANÁLISE DETALHADA DE SEGMENTAÇÃO DE CLIENTES E PADRÕES DE COMPRA ===\\n\\n\")\n",
    "    \n",
    "    # Seção 1: Visão Geral\n",
    "    f.write(\"1. VISÃO GERAL DA BASE DE CLIENTES\\n\")\n",
    "    f.write(\"-\" * 50 + \"\\n\")\n",
    "    f.write(f\"Total de clientes analisados: {len(rfm)}\\n\")\n",
    "    f.write(f\"Período de análise: {df['DataFatura'].min().strftime('%d/%m/%Y')} a {df['DataFatura'].max().strftime('%d/%m/%Y')}\\n\")\n",
    "    f.write(f\"Valor total transacionado: R$ {df['ValorTotalFatura'].sum():,.2f}\\n\\n\")\n",
    "\n",
    "    # Seção 2: Análise por Segmento\n",
    "    f.write(\"2. ANÁLISE DETALHADA DOS SEGMENTOS\\n\")\n",
    "    f.write(\"-\" * 50 + \"\\n\")\n",
    "    \n",
    "    for cluster in sorted(cluster_names.keys()):\n",
    "        segment_data = rfm[rfm['Cluster'] == cluster]\n",
    "        nome_segmento = cluster_names[cluster]\n",
    "        \n",
    "        f.write(f\"\\nSegmento: {nome_segmento}\\n\")\n",
    "        f.write(f\"Quantidade de clientes: {len(segment_data)} ({len(segment_data)/len(rfm)*100:.1f}%)\\n\")\n",
    "        \n",
    "        # Características do segmento\n",
    "        f.write(\"\\nCaracterísticas principais:\\n\")\n",
    "        f.write(f\"- Recência média: {segment_data['Recencia'].mean():.1f} dias\\n\")\n",
    "        f.write(f\"- Frequência média: {segment_data['Frequencia'].mean():.1f} compras\\n\")\n",
    "        f.write(f\"- Valor médio gasto: R$ {segment_data['ValorMonetario'].mean():,.2f}\\n\")\n",
    "        f.write(f\"- Valor médio por compra: R$ {segment_data['CompraMediaPorVisita'].mean():,.2f}\\n\")\n",
    "        f.write(f\"- Média de dias entre compras: {segment_data['DiasEntreCompras'].mean():.1f}\\n\")\n",
    "        \n",
    "        # Recomendações específicas por segmento\n",
    "        f.write(\"\\nInsights e Recomendações:\\n\")\n",
    "        if nome_segmento == 'Champions':\n",
    "            f.write(\"- Clientes de alto valor que compram frequentemente e recentemente\\n\")\n",
    "            f.write(\"- Foco em retenção e programas de fidelidade premium\\n\")\n",
    "            f.write(\"- Oportunidade para desenvolvimento de produtos premium\\n\")\n",
    "            f.write(\"- Potenciais embaixadores da marca\\n\")\n",
    "        elif nome_segmento == 'Leais':\n",
    "            f.write(\"- Base sólida de clientes regulares\\n\")\n",
    "            f.write(\"- Potencial para aumentar frequência de compras\\n\")\n",
    "            f.write(\"- Implementar programa de recompensas por frequência\\n\")\n",
    "            f.write(\"- Foco em cross-selling de produtos complementares\\n\")\n",
    "        elif nome_segmento == 'Em Risco':\n",
    "            f.write(\"- Sinais de diminuição na frequência de compras\\n\")\n",
    "            f.write(\"- Necessidade de ação rápida para reengajamento\\n\")\n",
    "            f.write(\"- Pesquisar motivos de afastamento\\n\")\n",
    "            f.write(\"- Ofertas personalizadas baseadas em histórico de compras\\n\")\n",
    "        else:  # Perdidos\n",
    "            f.write(\"- Alta probabilidade de churn\\n\")\n",
    "            f.write(\"- Campanha de reativação com ofertas agressivas\\n\")\n",
    "            f.write(\"- Análise de causas de abandono\\n\")\n",
    "            f.write(\"- Considerar remarketing específico\\n\")\n",
    "\n",
    "    # Seção 3: Análise de Market Basket\n",
    "    f.write(\"\\n3. PADRÕES DE COMPRA POR SEGMENTO\\n\")\n",
    "    f.write(\"-\" * 50 + \"\\n\")\n",
    "    \n",
    "    if 'regras_todas' in locals():\n",
    "        for segmento in regras_todas['Segmento'].unique():\n",
    "            f.write(f\"\\nPadrões de Compra - Segmento {segmento}\\n\")\n",
    "            regras_segmento = regras_todas[regras_todas['Segmento'] == segmento].sort_values('lift', ascending=False).head(5)\n",
    "            \n",
    "            if not regras_segmento.empty:\n",
    "                f.write(\"\\nPrincipais associações de produtos:\\n\")\n",
    "                for _, regra in regras_segmento.iterrows():\n",
    "                    antecedents = ', '.join(list(regra['antecedents']))\n",
    "                    consequents = ', '.join(list(regra['consequents']))\n",
    "                    f.write(f\"\\n- Quando compram: {antecedents}\")\n",
    "                    f.write(f\"\\n  Também tendem a comprar: {consequents}\")\n",
    "                    f.write(f\"\\n  Confiança: {regra['confidence']:.2%}\")\n",
    "                    f.write(f\"\\n  Lift: {regra['lift']:.2f}\\n\")\n",
    "\n",
    "    # Seção 4: Recomendações Estratégicas\n",
    "    f.write(\"\\n4. RECOMENDAÇÕES ESTRATÉGICAS GERAIS\\n\")\n",
    "    f.write(\"-\" * 50 + \"\\n\")\n",
    "    f.write(\"\\nCurto Prazo (1-3 meses):\\n\")\n",
    "    f.write(\"- Implementar campanhas de reativação para segmento 'Perdidos'\\n\")\n",
    "    f.write(\"- Desenvolver programa de fidelidade segmentado\\n\")\n",
    "    f.write(\"- Ajustar mix de produtos baseado nas análises de market basket\\n\")\n",
    "    \n",
    "    f.write(\"\\nMédio Prazo (3-6 meses):\\n\")\n",
    "    f.write(\"- Estabelecer metas de migração entre segmentos\\n\")\n",
    "    f.write(\"- Desenvolver estratégias de up-selling para segmento 'Leais'\\n\")\n",
    "    f.write(\"- Implementar sistema de alertas para clientes em risco de churn\\n\")\n",
    "    \n",
    "    f.write(\"\\nLongo Prazo (6-12 meses):\\n\")\n",
    "    f.write(\"- Revisar e ajustar segmentação baseado em resultados\\n\")\n",
    "    f.write(\"- Desenvolver produtos/serviços específicos por segmento\\n\")\n",
    "    f.write(\"- Estabelecer programa de customer success para Champions\\n\")\n",
    "\n",
    "print(\"Relatório de insights gerado com sucesso!\")\n",
    "\n",
    "# === SALVAMENTO DO MODELO E METADATA ===\n",
    "\n",
    "print(\"\\nIniciando processo de salvamento do modelo e metadata...\")\n",
    "\n",
    "# 1. Preparar informações do modelo\n",
    "model_info = {\n",
    "    # Configurações básicas do modelo\n",
    "    'n_clusters': kmeans.n_clusters,\n",
    "    'feature_names': ['Recencia', 'Frequencia', 'ValorMonetario'],\n",
    "    'data_treinamento': str(datetime.now()),\n",
    "    \n",
    "    # Informações sobre o pré-processamento\n",
    "    'pre_processamento': {\n",
    "        'outliers_tratamento': {\n",
    "            'metodo': 'clipping',\n",
    "            'percentis': {'inferior': 0.01, 'superior': 0.99}\n",
    "        },\n",
    "        'transformacoes': ['log1p', 'StandardScaler'],\n",
    "        'scaler_info': {\n",
    "            'mean': scaler.mean_.tolist(),\n",
    "            'scale': scaler.scale_.tolist()\n",
    "        }\n",
    "    },\n",
    "    \n",
    "    # Métricas de avaliação\n",
    "    'metricas': {\n",
    "        'inertia': float(kmeans.inertia_),\n",
    "        'silhouette': float(silhouette_avg),\n",
    "        'tamanho_clusters': rfm['Cluster'].value_counts().to_dict(),\n",
    "        'metricas_por_cluster': cluster_metrics.to_dict()\n",
    "    },\n",
    "    \n",
    "    # Parâmetros do modelo\n",
    "    'parametros': {\n",
    "        'n_init': kmeans.n_init,\n",
    "        'random_state': 42,\n",
    "        'algorithm': kmeans.algorithm\n",
    "    },\n",
    "    \n",
    "    # Mapeamento dos segmentos\n",
    "    'segmentos': {\n",
    "        'Champions': 'Cluster 0: Clientes de alto valor, frequentes e recentes',\n",
    "        'Leais': 'Cluster 1: Base sólida de clientes regulares',\n",
    "        'Em Risco': 'Cluster 2: Clientes com sinais de afastamento',\n",
    "        'Perdidos': 'Cluster 3: Alta probabilidade de churn'\n",
    "    },\n",
    "    \n",
    "    # Informações sobre os dados\n",
    "    'dados': {\n",
    "        'total_clientes': len(rfm),\n",
    "        'periodo_analise': {\n",
    "            'inicio': df['DataFatura'].min().strftime('%Y-%m-%d'),\n",
    "            'fim': df['DataFatura'].max().strftime('%Y-%m-%d')\n",
    "        }\n",
    "    },\n",
    "    \n",
    "    # Informações do Market Basket\n",
    "    'market_basket': metricas_market_basket\n",
    "}\n",
    "\n",
    "# 2. Salvar modelo e componentes\n",
    "try:\n",
    "    # Salvar o modelo K-means\n",
    "    dump(kmeans, 'models/customer_segments.joblib')\n",
    "    print(\"Modelo K-means salvo com sucesso!\")\n",
    "    \n",
    "    # Salvar o scaler\n",
    "    dump(scaler, 'models/scaler.joblib')\n",
    "    print(\"Scaler salvo com sucesso!\")\n",
    "    \n",
    "    # Salvar metadata do modelo\n",
    "    dump(model_info, 'models/model_info.joblib')\n",
    "    print(\"Metadata do modelo salva com sucesso!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Erro ao salvar arquivos: {e}\")\n",
    "\n",
    "# 3. Teste de carregamento\n",
    "print(\"\\nRealizando testes de carregamento...\")\n",
    "try:\n",
    "    # Carregar e testar modelo\n",
    "    modelo_teste = load('models/customer_segments.joblib')\n",
    "    scaler_teste = load('models/scaler.joblib')\n",
    "    info_teste = load('models/model_info.joblib')\n",
    "    \n",
    "    print(\"Testes de carregamento realizados com sucesso!\")\n",
    "    print(f\"Número de clusters do modelo carregado: {modelo_teste.n_clusters}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Erro nos testes de carregamento: {e}\")\n",
    "\n",
    "# 4. Resumo final\n",
    "print(\"\\n=== RESUMO DA EXECUÇÃO ===\")\n",
    "print(f\"Total de registros processados: {len(df)}\")\n",
    "print(f\"Total de clientes segmentados: {len(rfm)}\")\n",
    "print(f\"Arquivo de insights gerado: documents/analise_detalhada_clientes.txt\")\n",
    "print(f\"Modelos e metadata salvos em: /models/\")\n",
    "print(\"\\nProcesso concluído com sucesso!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
